{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa56db7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis using AdaBoostClassifier\n",
    "\n",
    "# This script implements sentiment analysis using AdaBoost with optimized hyperparameters.\n",
    "# Key optimizations: Pipeline for tuning DecisionTreeClassifier parameters (max_depth), comprehensive hyperparameter grid, enhanced TF-IDF with bigrams, stratified split, cross-validation with parallel processing.\n",
    "\n",
    "# Section 1: Setup and Library Imports\n",
    "# ------------------------------------------- \n",
    "# Install required packages: pip install pandas numpy matplotlib seaborn wordcloud nltk scikit-learn ipython\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc)\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display\n",
    "\n",
    "# Download required NLTK data for text preprocessing\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "print(\"✅ All libraries imported successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26373b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Section 2: Load Dataset\n",
    "# ------------------------------------------------\n",
    "# Load the 'train.csv' dataset from local file structure.\n",
    "try:\n",
    "    df = pd.read_csv('/content/train.csv', encoding='ISO-8859-1')\n",
    "    print(\"\\n✅ Dataset '/train.csv' loaded successfully.\")\n",
    "    display(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n❌ '/train.csv' not found. Please ensure the file is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238cc4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Section 3: Exploratory Data Analysis (EDA)\n",
    "# -------------------------------------------\n",
    "if 'df' in locals():\n",
    "    # Perform extended EDA: Clean, add features.\n",
    "    df.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "    df['text_length'] = df['text'].astype(str).apply(len)\n",
    "    df['word_count'] = df['text'].astype(str).apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "    print(\"\\nStatistical Summary of New Features:\")\n",
    "    display(df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27c671",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Section 4: Advanced Visualizations (5 Required Types)\n",
    "# ------------------------------------------------------\n",
    "if 'df' in locals():\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    # 1. Pie Chart: Sentiment Distribution\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    df['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%',\n",
    "                                        colors=['skyblue', 'salmon', 'lightgray'],\n",
    "                                        wedgeprops={'edgecolor': 'black'})\n",
    "    plt.title('Pie Chart: Sentiment Distribution', fontsize=14)\n",
    "    plt.ylabel('')\n",
    "    plt.show()\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    print(\"Generated sentiment_distribution.png\")\n",
    "\n",
    "    # 2. Violin Plot: Text Length by Sentiment\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x='sentiment', y='text_length', data=df, palette=['skyblue', 'salmon', 'lightgray'])\n",
    "    plt.title('Violin Plot: Text Length by Sentiment', fontsize=14)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Text Length')\n",
    "    plt.show()\n",
    "    plt.savefig('text_length_violin.png')\n",
    "    plt.close()\n",
    "    print(\"Generated text_length_violin.png\")\n",
    "\n",
    "    # 3. Pair Plot: Correlations Between Features\n",
    "    sns.pairplot(df[['text_length', 'word_count', 'sentiment']], hue='sentiment',\n",
    "                 palette=['skyblue', 'salmon', 'lightgray'])\n",
    "    plt.suptitle('Pair Plot: Text Features by Sentiment', y=1.02)\n",
    "    plt.show()\n",
    "    plt.savefig('pair_plot.png')\n",
    "    plt.close()\n",
    "    print(\"Generated pair_plot.png\")\n",
    "\n",
    "    # 4. KDE Plot: Density of Word Counts by Sentiment\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for sent in df['sentiment'].unique():\n",
    "        sns.kdeplot(data=df[df['sentiment'] == sent], x='word_count', label=sent, fill=True, alpha=0.5)\n",
    "    plt.title('KDE Plot: Density of Word Counts by Sentiment', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('word_count_kde.png')\n",
    "    plt.close()\n",
    "    print(\"Generated word_count_kde.png\")\n",
    "\n",
    "    # 5. Word Clouds: Most Frequent Words per Sentiment\n",
    "    for sent in df['sentiment'].unique():\n",
    "        text = ' '.join(df[df['sentiment'] == sent]['text'].astype(str))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                              stopwords=STOPWORDS, collocations=False).generate(text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for: {sent.capitalize()} Sentiment', fontsize=16)\n",
    "        plt.show()\n",
    "        plt.savefig(f'word_cloud_{sent}.png')\n",
    "        plt.close()\n",
    "        print(f\"Generated word_cloud_{sent}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3a58d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Section 5: Data Preprocessing\n",
    "# -----------------------------\n",
    "if 'df' in locals():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        tokens = text.lower().split()\n",
    "        clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        return ' '.join(clean_tokens)\n",
    "\n",
    "    print(\"\\nPreprocessing text data...\")\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    print(\"✅ Preprocessing complete.\")\n",
    "\n",
    "    # Display a sample to verify\n",
    "    print(\"\\nSample Original Text:\\n\", df['text'].iloc[5])\n",
    "    print(\"\\nSample Cleaned Text:\\n\", df['cleaned_text'].iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12975980",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Section 6: Feature Engineering and Data Splitting\n",
    "# --------------------------------------------------\n",
    "if 'df' in locals():\n",
    "    X_text = df['cleaned_text']\n",
    "    y_labels = df['sentiment']\n",
    "\n",
    "    # TF-IDF vectorization with max_features and ngram_range\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_labels)\n",
    "    print(\"\\nLabel mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "    # Stratified split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"\\nData split into: \\nTraining set shape: {X_train.shape} \\nTesting set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fc0eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Section 7: Model Training - AdaBoost with Hyperparameter Tuning\n",
    "# ----------------------------------------------------------------\n",
    "if 'df' in locals():\n",
    "    # Pipeline for tuning base_estimator params\n",
    "    pipe = Pipeline([\n",
    "        ('classifier', AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Param grid: n_estimators, learning_rate, estimator max_depth\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.1, 0.5, 1.0],\n",
    "        'classifier__estimator__max_depth': [1, 2, 3]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    print(\"\\nStarting hyperparameter tuning with GridSearchCV...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"\\n✅ Tuning complete. Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fec9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Section 8: Model Evaluation\n",
    "# -----------------------------------------------------------------\n",
    "if 'df' in locals():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    roc_auc = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "    print(\"\\n--- Model Performance Metrics ---\")\n",
    "    print(f\"1. Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"2. Precision (Macro): {precision:.4f}\")\n",
    "    print(f\"3. Recall (Macro): {recall:.4f}\")\n",
    "    print(f\"4. F1-Score (Macro): {f1:.4f}\")\n",
    "    print(f\"5. ROC-AUC (One-vs-Rest): {roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "    # Confusion Matrix Heatmap\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    print(\"Generated confusion_matrix.png\")\n",
    "\n",
    "    # ROC Curves (One-vs-Rest)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color, class_name in zip(range(len(le.classes_)), colors, le.classes_):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc_val = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                 label=f'ROC curve for {class_name} (area = {roc_auc_val:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-Class ROC Curves (One-vs-Rest)', fontsize=14)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    plt.savefig('roc_curves.png')\n",
    "    plt.close()\n",
    "    print(\"Generated roc_curves.png\")\n",
    "\n",
    "    print(\"\\n--- End of Notebook ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
